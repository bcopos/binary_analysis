\documentclass{acm_proc_article-sp}
\usepackage{graphicx}

% definitions
\def \tool {InputFinder}  

\begin{document}

\title{InputFinder: Reverse Engineering Closed Binaries via Side Channels}
\numberofauthors{2}
\author{
\alignauthor
Bogdan Copos\\
	\affaddr{University of California, Davis}\\
	\email{bcopos@ucdavis.edu}
\alignauthor
Praveen Murthy\\
	\affaddr{Fujitsu Laboratories of America}\\
	\email{praveen.murthy@us.fujitsu.com}
}

\maketitle
\begin{abstract}

We propose a new approach for the autonomous generation of test suites for dynamic analysis of closed x86 binaries and build a tool called \tool{} for determining input for closed binarie and identifying the appropriate protocol (if any). 
We show that (a) our technique can autonomously detect input, (b) one can easily leverage the results from our tool to build a sophisticated dynamic analysis tool, (c) the evaluation of \tool{} are on par and often exceed prior works designed for autonomous testing of programs, which we demonstrate by comparing \tool{}'s results with those of prior works on a set of binaries from XXX.
Since \tool{}'s analysis 

% OR

The effectiveness of dynamic analysis techniques depends heavily on the completeness of the test suite applied during the analysis process.
Test suites are often composed by developers and aim at covering (and testing) all of the functionality of a software system.
However, test suites may still not be complete, if they exist at all.
To date, only two methods exist for automatically generating test input for closed binaries: black-box fuzzing and symbolic execution.
Despite previous success methods in identifying bugs, both techniques have discouraging limitations.
In this paper, we present a (novel?) method for automatically generating test suites for closed x86 binaries.
To assess its effectiveness, we have developed \tool{} and tested it against XXXX binaries from XXXXX.
With a success rate of XXXX, \tool{} 

\end{abstract}

\section{Introduction}

%introduction
Software testing is an expensive and labor intensive process.
For many years, researchers have explored both static and dynamic methods for analyzing programs \cite{smartfuzzer}.
While static analysis is thought to be more thorough, dynamic testing is often preferred due to its time efficiency.
However, dynamic testing heavily depends on availability of input or test suites.

%motivation
Without complete test suites, the program cannot be properly tested and bugs may be omitted.
While valid input can be learned from software documentation or source code, when such information is not available or complete, other methods must be used.
Automated dynamic analysis techniques such as fuzzing and symbolic execution attempt to generate input for a given program.
Currently these are the only two options available and both approaches present discouraging limitations.

In this paper, we introduce a new method for generating valid user-input for closed binaries. 
Our method is composed of two main components.
The first component exploits side channels to build valid input for closed binaries.
Specifically, as the program is provided various input, \tool{} records the number of instructions retired by the program for each input and compares the recorded value to learn the program's expected input.
The second component uses the discovered input to build a protocol state machine associated with the tested program.
This enables the generation of a more thorough test suite.

%We show that by measuring the number of user-land instructions retired and comparing them between different executions of the program with various input, it is possible to learn the program's expected input.
%We also present an implementation of our method, \tool{}.

The main contributions of this paper are as follows:
\begin{itemize}
	\item We introduce a new approach to automatically generate input valid strings for unknown, closed-source binaries by leveraging hardware instruction counters. Our method relies on observing changes in the number of instructions executed by programs during their input validation process to learn valid input strings.
	\item We develop a method for generating protocol state machines using the valid input previously found.
	\item We implement the techniques presented and evaluate the tool using binaries published by DARPA as part of the 2014 Cyber Grand Challenge competition. Additionally, we compare our approach to the manual process used by a security specialist to crack a binary as part of a job interview.
\end{itemize}

The paper is organized as follows.
Section 2 covers background information.
Section 3 discusses related works and differences with respect to the presented method.
In Section 4 our work is explained in detail. First, the method for finding valid input is described. Then we introduce our method for building protocol state machines using the discovered input.
Section 5 describes evaluation of the work presented and the results.
In Section 6 we discuss future work and limitations of the presented work.

\section{Background} \label{background}
 
\subsection{Side Channels} \label{bg_sidechannels}

Side channels are streams of information that can be retrieved from the hardware of a device running a given program.
This information can be used to learn about the program's internal components.
Side channels have been widely explored by security researchers in order to exploit weaknesses in programs \cite{weinbergside, schindler2002combined, genkin2014rsa, genkinstealing}.
In cryptography, side channel attacks exploit such information channels to defeat crypto systems \cite{zhou2005side, black2002black, okeya2006side}.
Recently, side channel attacks, such as OpenSSL's CREAM cache timing attack have made headlines \cite{}.
Similar to CREAM and other side channel attacks, the technique discussed in this paper exploits such information channels to generate valid input for binaries.

\subsection{Hardware counter registers} \label{bg_hardwarereg}

Most modern microprocessors have a set of special-purpose registers used to count hardware events.
Each register, or counter, can be programmed to measure specific events such as cache misses, floating point operations, and even instructions executed.
These hardware performance counters are often used by software developers to analyze their programs and improve on running time and efficiency.
However, as we will show, such performance counters can also be exploited as side channels.
Utilities, such as Linux/Unix `perf', take advantage of such hardware counters in order to provide developers with useful information about a program's performance.
`perf' allows developers to specify the desired hardware event they wish to measure.
Once events have been selected, `perf' interacts with the appropriate kernel modules to program the hardware counters accordingly.
In this work, we use `perf' to count the number of \textbf{user-land} instructions executed as a program is tested with various input.
The number of total instructions retired oscillates significantly for a binary from execution to execution.
This phenomena reflects fluctuations in instructions executed by the kernel depending on the state of the machine due to cache-misses and other factors.
Since we rely on this count, such variations are problematic.
However, experimentally, the number of user-land instructions only varies, on average, by one instruction, giving us a reliable counter.

\subsection{Execution/Dynamic Matching} \label{bg_dynamicsim}

In many application areas, including clone detection and software debugging, there is a need for comparing executions of a program (or multiple versions of a program).
One of the proposed approaches aimed at tackling this issue is dynamic matching.
Dynamic matching works by first collecting execution traces of different executions.
Such traces can collected through a number of ways.
One option includes the use of Pin \cite{pintool}.
Pin is a framework for building customized binary analysis tools and can be used to design a tool which creates a transcript of the instructions executed by a program.
Once the traces are collected, they are analyze and mappings between instructions of different executions are produced.
These mappings can be used to understand the relationship between two executions (or two versions of a program).
In this work, we rely on the concept of dynamic matching and the Pin framework to develop a protocol state machine generator for closed binaries.

\section{Previous Work} \label{prevwork}

Dynamic analysis has been extensively studied in academia and is used widely in the industry by security specialists.
Two of the main approaches to finding bugs are fuzzing and symbolic execution.

Fuzzers are a popular choice amongst software testers.
They are easy to use, easy to implement, and have produced surprising results in the past \cite{millerfuzz, millerfuzzrevisited}.
There are two classes of fuzzers: black box fuzzers and white box fuzzers.
In black box fuzzing, the program is watched as it is tested with randomly generated input.
If the program crashes, the input(s) are recorded and reported.
No program specifications or knowledge of the input format is available in such cases.
Since black box fuzzing is completely random, its effectiveness is limited, especially with respect to coverage.
On the other hand, in white box fuzzing, inputs are used to gather symbolic constraints which are in turn systematically negated and solved to produce more inputs.
The approach introduced in this paper is similar to white box fuzzing in that information from the program's execution is used to craft valid input.
White box fuzzing differs from black box fuzzing in that it uses the program's feedback to make inferences about valid input and use constraint solvers to generate more input.
White box fuzzing can attain better code coverage but can be slow and is crippled by innate limitations of constraint solvers.
One of the works utilizing white box fuzzing is \cite{molnar2009dynamic} which introduces SmartFuzz, a dynamic test generation tool.
SmartFuzz works by using symbolic execution to learn about the program's input and passing that information to a fuzzer to generate test cases.
The main difference between our work and SmartFuzz is that our work does not use symbolic execution to gain insight into the format of the input.
Similarly to SmartFuzz, the output of our work can be used by a fuzzer to generate test cases.
Another related work is presented in \cite{smartfuzzer}, where the authors use a combination of static and dynamic analysis to build a smart fuzzer.
Their work uses static analysis to collect preliminary information such as an approximation of the inter procedural control flow graph.
Having gathered static information, their dynamic engine executes the program while monitoring dependencies between the input and the dynamic control flow path experienced by the program.
Their work also takes advantage of a constraint solver that given a path of the program generates input which forces the program to follow the chosen path.
Our work \tool{} differs from \cite{smartfuzzer} in several ways.
In our proposed methods, we do not use neither static analysis nor constraint solver.
Instead we rely only on information extracted from the execution of the program. 

Symbolic execution is another method used for generating valid input.
In symbolic execution, an interpreter gathers constraints related to input throughout the program's execution, rather than using the provided input.
At the end of the execution, a constraint solver takes the constraints gathered and outputs valid program input.
Symbolic execution is studied extensively and can be effective \cite{symbolicexecution, exesymex}.
Similar to white box fuzzing, symbolic execution has a fundamental limitation with respect to constraint solving.
While symbolic execution engines perform well with linear constraints, non-linear constraints greatly impact efficiency.
One of the most important works in this area is described in \cite{exesymex} which introduces EXE, a dynamic analysis tool capable of generating inputs that crash programs by running the programs with symbolic input.
Unlike symbolic execution, the approach presented in this paper is not hindered by constraint solvers' limitations and does not require any instrumentation or source code.
\tool{} does not execute the program with symbolic input but rather it uses concolic input and information about the execution of the program to craft valid input.

There have also been numerous works studying and applying dynamic similarity.
In \cite{blanketexec}, the authors propose blanket execution, a novel dynamic equivalence testing technique.
Blanket execution is designed to collect information about the execution of functions, including side effects, which is then used for similarity testing.
Both our approach and blanket execution use a dynamic approach to gather information about the program.
However, our approach aims at finding similarities in executions of entire programs (not just functions) with various inputs.
This allows us to discover differences in inputs which experience different control flow paths.

Another area of research relevant to this paper is protocol reverse engineering.
An important result in this area is \cite{prospex} which introduces a technique for extracting protocol specifications from packet captures and automatically generating input which can be used by a stateful fuzzer.
There are two notable difference between Prospex and our work.
While the end result is similar, \tool{} assumes no prior knowledge about the tested program and hence it does not utilize packet captures for protocol specification extraction.
Instead, \tool{} infers the protocol through dynamic analysis by automatically finding input and using that to generate a protocol state machine.
To date, there are no protocol reverse engineering techniques which assume no prior knowledge about the program's input.

\section{Motivating Example???} \label{example}



\section{Methodology} \label{methodology}

% add some high level intro for both finding input and psm stuff
\tool{} is capable of find input strings for a closed binary and generating a protocol state machine using the crafted input.

The majority of software programs accept input, perform transformations on the input and output results.
However, most programs do not accept completely random input.
Input usually passes through an input validation filter.
Such filters are snippets of code responsible for distinguishing between valid or good input (i.e. input the program was constructed to understand and accept) and bad input (i.e. input which is not useful or does not follow the desired format).
Although validation mechanisms can be very sophisticated, they are often a combination of string comparisons and conditional statements.
Our method exploits changes in the number of user-land instructions retired during multiple executions with varying inputs to make inferences about the program's validation mechanism.
These changes reflect different execution paths of the program as a response to both valid and invalid input.
Assuming most input is invalid, our method observes differences in the number of instructions executed to detect input which passes the validation mechanism.


\subsection{Finding Valid Input}

\begin{figure*}[t]
\centering
\includegraphics[height=2.5in,width=6.5in]{architecture.png}
\label{fig:architecture}
\caption{}
\end{figure*}

Figure~\ref{fig:architecture} shows the overall architecture of the finding input component of \tool{}.
The process is recursive and is composed of two steps.

% printable character or all charactersls?
The steps of the process for finding valid input are outlined below. Starting with an empty string, \textit{s}:
\begin{enumerate}
\item \tool{} executes the program once for every printable character and records the number of user-land instructions executed by the program with the given input. The input is a string concatenation of the string \textit{s} and the current printable character. The Unix `perf' utility is used for monitoring the number of instructions executed. Note that \tool{} does not record the total number of instructions but rather user-land instructions. The reason for this, as explained in Section \ref{bg_hardwarereg}, is that the total number of instructions executed can vary greatly from execution to execution due to operating system factors (e.g. branch prediction, cache). The Expect tool is used for automating the program interaction.
\item Once all executions have been completed (one for each character), the mode number of instructions retired is computed across all executions. The mode is defined as the value that appears most often in the set of recordings.
\item Next is the filtering stage which is responsible for identifying characters likely to be part of a valid input string at the current index. Under the assumption that most characters are not valid for the current index, \tool{} gathers all characters with the number of instructions executed outside the range of the mode +/- an epsilon value. These characters represent valid characters for the current index of the input string. The epsilon value accounts for small variations in the hardware counters. While the initial reaction is to expect a valid character to result in more instructions executed than a non-valid character, this is not always the case. As discussed earlier, the input validation mechanism can be complex and can vary in behavior. The validation mechanism may verify input against all accepted input strings before denying the provided input. Such a scenario results in more instructions executed for invalid input than valid input.
\item For each valid character \textit{c}, \tool{} forks with every child process repeating steps 1-4, appending \textit{c} to the end of the string \textit{s}. If no valid characters are found, the end of a valid input string has been reached and the string is recorded. If the current index is 0 and no valid characters have been found, the given binary has no predefined input commands.
\end{enumerate}

\begin{figure*}[t]
\centering
\includegraphics[height=2in,width=6.5in]{string_builder.png}
\label{fig:string_builder}
\caption{}
\end{figure*}

Figure~\ref{fig:string_builder} depicts the results of the above process over the span of 5 time stages.
As seen in the figure, at time \textit{T0}, \tool{} begins with an empty string.
At time \textit{T1}, \tool{} finds valid characters for index 0 of the input and is forked into \textit{n} child processes where \textit{n} is the number of valid characters found.
The process continues until all child processes are done.


\subsection{Finding Desired Input Size}

The same side channels can be used to detect the expected size of the program's input or even the program's input buffer.
Most programs are designed to read a predefined number of bytes from the source of input and store them in a buffer.
Depending on the implementation, when a program is given more input than the size of the buffer, additional instructions may be executed.
Regardless of the number of instructions executed, the behavior of the program changes.
A program may exit when the number of provided bytes is larger than expected.
Similarly, once the input buffer size limit has been reach, a program may check to see if the next byte is a newline or EOF.
Some methods, such as C's `fgets()' function, read character by character [EXPLAIN]
Our method observes such differences in the number of user-instructions retired as the program is given increasingly larger input.

To start, \tool{} finds the set of all invalid characters for index 0 of all input strings and picks one at random.
The character selected is used to build invalid input strings of various sizes.
It is crucial to use invalid input to avoid noisy fluctuations in the number of instructions executed due to valid input processing.
Initially, the program is executed with short length input strings (i.e. two and three characters).
The number of user-land instructions executed during these two executions are recorded.
These two values may be equal or differ, depending on the program's logic.
If the two values are equal [EQUAL]
The two values can also differ because... [DIFFERENT]

\subsection{Classifying Input}

Our method is also capable of classifying the input discovered.
Generated input strings are classified in three categories: alphabetic, numeric, alphanumeric.
Similarly, the classification algorithm detects special characters.
Once enough input strings have been identified, \tool{} uses the categorization data to generalize about the type of input the program accepts.
Such information is valuable especially in cases where the inputs discovered are all numeric or contain (one or more) special characters.
Having learned such information about the program, the testing platform (e.g. fuzzer) can be adjusted to proceed accordingly.

\subsection{Generating Protocol State Machine}

% TODO
% add diagram depicting process
% add explanation

\section{Evaluation and Results}

\subsection{Finding Input, Input Size and Classifying Input}
\tool{} was tested on [NUMBER] closed-source binaries to measure its effectiveness.
The results are summarized in Figure [FIGURE].
Out of the total [NUMBER] binaries, [NUMBER2] had predefined accepted input strings.
\tool{} found 100\% of the input strings for [NUMBER3]...
[CONTINUE].

Additionally, our method was tested on a binary used for technical interviews.
The binary was designed as a reverse engineering challenge for interviewers.
To pass the challenge, the interviewer has to crack the secret passcode to the program.
A blog post describes the manual process used to crack the password [CITE].
The manual process is tedious.
As described in the blog post, the cracking of the binary can take several hours and requires advanced skills and years of background knowledge.
\tool{} cracked the binary in a matter of minutes autonomously, proving itself as a useful tool not only finding valid input but also cracking passwords.

\subsection{Protocol State Machine Generation}

% TODO

\section{Future Work and Limitations}

While the method presented in this paper has many advantages, it also has a few limitations.
For certain programs that allow any input, such as an echo server, our method is not any more efficient than a fuzzer.
Additionally, our method cannot handle cases in which input is composed of multiple inter-dependent fields or when programs validate input fields out of order.

% TODO
% talk about finding arguments

\section{Conclusions}


\bibliographystyle{abbrv}
\bibliography{inputfinder}  % sigproc.bib is the name of the Bibliography in this case

\end{document}
