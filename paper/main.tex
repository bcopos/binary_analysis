\documentclass[10pt,twocolumn]{article}
\usepackage{multicol}
\usepackage{graphicx}

% definitions
\def \tool {InputFinder}  

\begin{document}

\twocolumn[{%
  \centering
  \large \textbf{InputFinder: Reverse Engineering Closed Binaries via Side Channels} \\[2em]
  Bogdan Copos, University of California, Davis\\
  Praveen Murthy, Fujitsu Laboratories of America \\[4em]
}]


\begin{abstract}

We propose a new approach for the autonomous generation of test suites for dynamic analysis of closed binaries and build a general tool called \tool{} for determining input for a closed binary and identifying the appropriate protocol (if any). 
We show that (a) our technique can autonomously detect input, (b) one can easily leverage the results from our tool to build a sophisticated dynamic analysis tool, (c) the evaluation of \tool{} are on par and often exceed prior works designed for autonomous testing of programs, which we demonstrate by comparing \tool{}'s results with those of prior works on a set of binaries from XXX.
Since \tool{}'s analysis 
%

The effectiveness of dynamic analysis techniques depends heavily on the completeness of the test suite applied during the analysis process.
Test suites are often composed by developers and aim at covering (and testing) all of the functionality of a software system.
However, test suites may still not be complete, if they exist at all.
To date, only two methods exist for automatically generating test input for closed binaries: black-box fuzzing and symbolic execution.
Despite previous success methods in identifying bugs, both techniques have discouraging limitations.
In this paper, we present a (novel?) method for automatically generating test suites for closed binaries.
To assess its effectiveness, we have developed \tool{} and tested it against XXXX binaries from XXXXX.
With a success rate of XXXX, \tool{} 

\end{abstract}

\section{Introduction}

%introduction
Software testing is an expensive and labor intensive process.
For many years, researchers have explored both static and dynamic methods for analyzing programs \cite{}.
While static analysis is thought to be more thorough, dynamic testing is often preferred due to its time efficiency.
However, dynamic testing heavily depends on availability of input or test suites.

%motivation
Without complete test suites, the program cannot be properly tested and bugs may be omitted.
While valid input can be learned from software documentation or source code, when such information is not available or complete, other methods must be used.
Automated dynamic analysis techniques such as fuzzing and symbolic execution attempt to generate input for a given program.
Currently these are the only two options available and both approaches present discouraging limitations.

In this paper, we introduce a new method for generating valid user-input for closed binaries. 
Our method is composed of two main components.
The first component exploits side channels to build valid input for closed binaries.
Specifically, as the program is provided various input, \tool{} records the number of instructions retired by the program for each input and compares the recorded value to learn the program's expected input.
The second component uses the discovered input to build a protocol state machine associated with the tested program.
This enables the generation of a more thorough test suite.

%We show that by measuring the number of user-land instructions retired and comparing them between different executions of the program with various input, it is possible to learn the program's expected input.
%We also present an implementation of our method, \tool{}.

The main contributions of this paper are as follows:
\begin{itemize}
	\item We introduce a new approach to automatically generate input valid strings for unknown, closed-source binaries by leveraging hardware instruction counters. Our method relies on observing changes in the number of instructions executed by programs during their input validation process to learn valid input strings.
	\item We develop a method for generating protocol state machines using the valid input previously found.
	\item We implement the techniques presented and evaluate the tool using binaries published by DARPA as part of the 2014 Cyber Grand Challenge competition. Additionally, we compare our approach to the manual process used by a security specialist to crack a binary as part of a job interview.
\end{itemize}

The paper is organized as follows.
Section 2 covers background information.
Section 3 discusses related works and differences with respect to the presented method.
In Section 4 our work is explained in detail. First, the method for finding valid input is described. Then we introduce our method for building protocol state machines using the discovered input.
Section 5 describes evaluation of the work presented and the results.
In Section 6 we discuss future work and limitations of the presented work.

\section{Background} \label{background}
 
\subsection{Side Channels} \label{bg_sidechannels}

Side channels are streams of information that can be retrieved from the hardware of a device running a given program.
This information can be used to learn about the program's internal components.
Side channels have been widely explored by security researchers in order to exploit weaknesses in programs.
In cryptography, side channel attacks exploit such information channels to defeat crypto systems.
Recently, side channel attacks, such as OpenSSL's CREAM cache timing attack have made headlines. [CITE]
Similar to CREAM and other side channel attacks, the technique discussed in this paper exploits such information channels to generate valid input for binaries.

\subsection{Hardware counter registers} \label{bg_hardwarereg}

Most modern microprocessors have a set of special-purpose registers used to count hardware events.
Each register, or counter, can be programmed to measure specific events such as cache misses, floating point operations, and even instructions executed.
Utilities, such as Linux/Unix `perf', take advantage of such hardware counters in order to provide developers with useful information about a program's performance.
`perf' is a Unix tool which allows developers to specify the desired hardware event they wish to measure.
Once events have been selected, `perf' interacts with kernel modules to program the hardware counters accordingly.
In this work, we use `perf' to count the number of \textbf{user-land} instructions executed as a program is tested with various input.
The number of total instructions retired oscillates significantly for a binary from execution to execution.
This phenomena reflects fluctuations in instructions executed by the kernel depending on the state of the machine due to cache-misses and other factors.
Since we rely on this count, such variations are problematic.
However, experimentally, the number of user-land instructions only varies, on average, by one instruction, giving us a reliable counter.

\subsection{Execution/Dynamic Similarity} \label{bg_dynamicsim}



\section{Previous Work} \label{prevwork}

Fuzzers are a popular choice amongst software testers.
They are easy to use, implement, and have produced surprising results in the past.
There are two classes of fuzzers: black box fuzzers and white box fuzzers.
In black box fuzzing, the program is watched as it is tested with randomly generated input.
If the program crashes, the input(s) are recorded and reported.
No program specifications or knowledge of the input format is available in such cases.
On the other hand, in white box fuzzing, inputs are used to gather symbolic constraints which are in turn systematically negated and solved to produce more inputs.
White box fuzzing differs from black box fuzzing in that it uses the program's feedback to make inferences about valid input and use constraint solvers to generate more input.
Both fuzzing approaches have limitations.
Since black box fuzzing is completely random, its effectiveness is limited, especially with respect to fully exercising programs.
White box fuzzing can attain better code coverage but can be slow and can be crippled by innate limitations of constraint solvers.
[MAYBE TALK ABOUT FUZZING RESULTS, CITATIONS]

Symbolic execution is another method used for generating valid input.
In symbolic execution, an interpreter gathers constraints related to input throughout the program's execution, rather than using the provided input.
At the end of the execution, a constraint solver takes the constraints gathered and outputs valid program input.
Symbolic execution is studied extensively and can be effective [CITE].
[TALK ABOUT ONE OF THE CITATIONS]
Similar to white box fuzzing, symbolic execution has a fundamental limitation with respect to constraint solving.
While symbolic execution engines perform well with linear constraints, non-linear constraints greatly impact efficiency.
Furthermore, there is no autonomous symbolic execution engine.
All symbolic executioners today require some instrumentation of the program to determine the program's input buffer.
Some symbolic execution engines even require source code, which is not always available to the tester.

Side channel attacks and citations\ldots

The method presented in this paper takes advantage of hardware counters to observe program's behavior and automatically build valid input.
As opposed to fuzzers, where input is randomly generated, this approach uses side channels to intelligently compose valid program input.
Also, unlike symbolic execution, this approach is not hindered by constraint solvers' limitations and does not require any instrumentation or source code.

\section{Methodology} \label{methodology}

% add some high level intro for both finding input and psm stuff
The majority of software programs accept input, perform transformations on the input and output results.
However, most programs do not accept completely random input.
Input usually passes through an input validation filter.
Such filters are snippets of code responsible for distinguishing between valid or good input (i.e. input the program was constructed to understand and accept) and bad input (i.e. input which is not useful or does not follow the desired format).
Although validation mechanisms can be very sophisticated, they are often a combination of string comparisons and conditional statements.
Our method exploits changes in the number of user-land instructions retired during multiple executions with varying inputs to make inferences about the program's validation mechanism.
These changes reflect different execution paths of the program as a response to both valid and invalid input.
Assuming most input is invalid, our method observes differences in the number of instructions executed to detect input which passes the validation mechanism.


\subsection{Finding Valid Input}

\begin{figure*}[t]
\centering
\includegraphics[height=2.5in,width=6.5in]{architecture.png}
\label{fig:architecture}
\caption{}
\end{figure*}

Figure~\ref{fig:architecture} shows the overall architecture of the finding input component of \tool{}.
The process is recursive and is composed of two steps.

% printable character or all charactersls?
The steps of the process for finding valid input are outlined below. Starting with an empty string, \textit{s}:
\begin{enumerate}
\item \tool{} executes the program once for every printable character and records the number of user-land instructions executed by the program with the given input. The input is a string concatenation of the string \textit{s} and the current printable character. The Unix `perf' utility is used for monitoring the number of instructions executed. Note that \tool{} does not record the total number of instructions but rather user-land instructions. The reason for this, as explained in Section \ref{bg_hardwarereg}, is that the total number of instructions executed can vary greatly from execution to execution due to operating system factors (e.g. branch prediction, cache). The Expect tool is used for automating the program interaction.
\item Once all executions have been completed (one for each character), the mode number of instructions retired is computed across all executions. The mode is defined as the value that appears most often in the set of recordings.
\item Next is the filtering stage which is responsible for identifying characters likely to be part of a valid input string at the current index. Under the assumption that most characters are not valid for the current index, \tool{} gathers all characters with the number of instructions executed outside the range of the mode +/- an epsilon value. These characters represent valid characters for the current index of the input string. The epsilon value accounts for small variations in the hardware counters. While the initial reaction is to expect a valid character to result in more instructions executed than a non-valid character, this is not always the case. As discussed earlier, the input validation mechanism can be complex and can vary in behavior. The validation mechanism may verify input against all accepted input strings before denying the provided input. Such a scenario results in more instructions executed for invalid input than valid input.
\item For each valid character \textit{c}, \tool{} forks with every child process repeating steps 1-4, appending \textit{c} to the end of the string \textit{s}. If no valid characters are found, the end of a valid input string has been reached and the string is recorded. If the current index is 0 and no valid characters have been found, the given binary has no predefined input commands.
\end{enumerate}

Figure~\ref{fig:string_builder} depicts the results of the above process for 5 time stages.
As seen in the figure, at time \textit{T0}, \tool{} begins with an empty string.
At time \textit{T1}, \tool{} finds valid characters for index 0 of the input and is forked into \textit{n} child processes where \textit{n} is the number of valid characters found.
The process continues until all child processes are done. 


%\subsubsection{Finding Valid Characters}
%The process of finding valid characters for a given index is composed of two stages: the testing stage and the filtering stage.
%During the testing stage, the program is executed with `perf' and being provided as input, all of the printable characters, one at a time.
%Once the program has processed the input and is waiting for more input, the program is stopped.
%The automatic interaction with the program is performed using `Expect', an extension to the `Tcl' scripting language [CITE]
%In our implementation, each printable character results in a different program execution.
%At the end of each execution, the number of instructions retired is outputted by `perf'.
%This information is saved in a dictionary as a tuple of the format ( <printable character> , <number of instructions> ).
%As mentioned earlier, the differences in the number of instructions between various printable characters reflect different execution paths taken during the validation process.

%The filtering stage is responsible for identifying characters which are highly likely to be part of a valid input string at a given index.
%Using the dictionary data obtained from the testing stage, the number of instructions retired are analyzed.
%First, the mode of the number of instructions executed is calculated.
%The filtering of possible valid characters for a given index \textit{i}, of an input string \textit{s}, is based on the assumption that the majority of printable characters \textit{c} do not belong at index \textit{i} of string \textit{s} (i.e. \textit{c} $\notin$ \textit{s[i]}).
%Therefore, valid characters are defined as any printable character for which the number of instructions retired is outside the mode.

%Building valid input strings is a recursive process.
%Starting with an empty string and index 0, \tool{} tries to find valid characters at each index and incrementally builds valid input strings.
%More specifically, using the character finding technique described previously, relevant characters for index 0 are identified.
%The task is forked \textit{n} times, where \textit{n} is the number of characters found at the current index (i.e. index 0).
%Each child task is assigned one of the \textit{n} characters and is responsible for finding valid characters at the next index.
%These steps are repeated until no valid characters are found, marking the end of the input string.
%Figure~\ref{fig:string_builder} depicts the process.

\subsubsection{Building Input Strings}

\begin{figure*}[t]
\centering
\includegraphics[height=2in,width=6.5in]{string_builder.png}
\label{fig:string_builder}
\caption{}
\end{figure*}



\subsection{Finding Desired Input Size}

The same side channels can be used to detect the expected size of the program's input or even the program's input buffer.
Most programs are designed to read a predefined number of bytes from the source of input and store them in a buffer.
Depending on the implementation, when a program is given more input than the size of the buffer, additional instructions may be executed.
Regardless of the number of instructions executed, the behavior of the program changes.
A program may exit when the number of provided bytes is larger than expected.
Similarly, once the input buffer size limit has been reach, a program may check to see if the next byte is a newline or EOF.
Some methods, such as C's `fgets()' function, read character by character [EXPLAIN]
Our method observes such differences in the number of user-instructions retired as the program is given increasingly larger input.

To start, \tool{} finds the set of all invalid characters for index 0 of all input strings and picks one at random.
The character selected is used to build invalid input strings of various sizes.
It is crucial to use invalid input to avoid noisy fluctuations in the number of instructions executed due to valid input processing.
Initially, the program is executed with short length input strings (i.e. two and three characters).
The number of user-land instructions executed during these two executions are recorded.
These two values may be equal or differ, depending on the program's logic.
If the two values are equal [EQUAL]
The two values can also differ because... [DIFFERENT]

\subsection{Classifying Input}

Our method is also capable of classifying the input discovered.
Generated input strings are classified in three categories: alphabetic, numeric, alphanumeric.
Similarly, the classification algorithm detects special characters.
Once enough input strings have been identified, \tool{} uses the categorization data to generalize about the type of input the program accepts.
Such information is valuable especially in cases where the inputs discovered are all numeric or contain (one or more) special characters.
Having learned such information about the program, the testing platform (e.g. fuzzer) can be adjusted to proceed accordingly.

\subsection{Generating Protocol State Machine}

\section{Evaluation and Results}

\tool{} was tested on [NUMBER] closed-source binaries to measure its effectiveness.
The results are summarized in Figure [FIGURE].
Out of the total [NUMBER] binaries, [NUMBER2] had predefined accepted input strings.
\tool{} found 100\% of the input strings for [NUMBER3]...
[CONTINUE].

Additionally, our method was tested on a binary used for technical interviews.
The binary was designed as a reverse engineering challenge for interviewers.
To pass the challenge, the interviewer has to crack the secret passcode to the program.
A blog post describes the manual process used to crack the password [CITE].
The manual process is tedious.
As described in the blog post, the cracking of the binary can take several hours and requires advanced skills and years of background knowledge.
\tool{} cracked the binary in a matter of minutes automatically, proving itself as a useful tool not only finding valid input but also cracking passwords.

\section{Future Work and Limitations}

While the method presented in this paper has many advantages, it also has a few limitations.
For certain programs that allow any input, such as an echo server, our method is not any more efficient than a fuzzer.
Additionally, our method cannot handle cases in which input is composed of multiple inter-dependent fields or when programs validate input fields out of order.

\section{Conclusions}



\end{document}
